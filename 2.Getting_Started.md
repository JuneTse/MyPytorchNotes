##  pytorch 入门
* [PyTorch是什么](#1)
* [Tensors](#2)
* [Operations](#3)
* [与Numpy的桥接](#4)

### 1. PyTorch是什么？ <span id="1"></span>

它是一个基于Python的科学计算包:

> * 替代numpy来使用GPUs
> * 深度学习研究平台

### 2. Tensors <span id="2"></span>

> * Tensors与numpy的ndarrays相似
> * 另外，Tensors可以使用GPU加速计算

> * `torch.Tensor`是一个多维的矩阵，包含单一类型数据的元素
> * Tensor数据类型为CPU有7种，GPU有8种: (CPU的tensor定义为`torch.cuda.XX`)
> > * `torch.FloatTensor`  : 32-bit 浮点数
> > * `torch.DoubleTensor` : 64-bit 浮点数
> > * `torch.HalfTensor` : 16-bit 浮点数
> > * `torch.ByteTensor` : 8-bit  整数 (unsigned)
> > * `torch.CharTensor` :  8-bit 整数(signed)
> > * `torch.ShortTensor` : 16-bit整数 (signed)
> > * `torch.IntTensor`: 32-bit 整数(signed)
> > * `torch.LongTensor` : 64-bit 整数
>
>* `torch.Tensor` 另一种定义torch张量的方式，默认类型为`torch.FloatTensor` 

1. 导入pytorch:  
		
		import torch

2.  构建5x3的矩阵， 未初始化:
		
		x=torch.Tensor(5,3)
3. 可以从Python的`list`或sequence来构建tensor:

		x=torch.FloatTensor([[1,2,3], [4,5,6]])
4. 构建一个随机初始化的矩阵：

		x=torch.rand(5,3)

5. 获取tensor的size:

		size=x.size()

> 返回的是`torch.Size`类型，实际上是个tuple

### 3. Operations <span id="3"> </span>

运算操作有多个语法结构，比如：

* 加法

		y=torch.rand(5,3)
		#语法1
		print(x+y)
		#语法2
		print(torch.add(x,y))
		#语法3： 结果赋给输出tensor
		result=torch.Tensor(5,3)
		print(torch.add(x,y,out=result))
		print(result)
		#in-place: 把x加到y中
		y.add_(x)
		print(y)

> 任何原地改变一个tensor的操作，后面都有一个后缀`_`,比如`x.copy_(y)`, `x.t_()`, 会改变`x`

* 可以使用类似numpy的索引来切分tensor

		print(x[:,1])
* 更多操作

> * 有100多种Tensor operations
> * 包括转置，索引，切片，数学操作， 线性代数，随机数等
> * [http://pytorch.org/docs/master/torch.html](http://pytorch.org/docs/master/torch.html)

### 4. 与Numpy 桥接

>* torch Tensor和numpy数组之间的转换是轻而易举的。
>* torch Tensor和numpy数组共享底层内存地址，改变一个就会改变另一个

* torch Tensor转换成numpy数组

		a=torch.ones(5)
		print(a)
		b=a.numpy()
		print(b)
		#改变a,b也会改变
		a.add_(1)
		print(a)
		print(b)

* torch Tensor转换成numpy Array

		import numpy as np
		a=np.ones(5)
		b=torch.from_numpy(a)
		np.add(a,1,out=a)
		print(a)
		print(b)

### 5. CUDA Tensors

> 可以使用`.cuda()`函数把Tensors转移到GPU上

		#let us run this cell only if CUDA is available
		if torch.cuda.is_available():
			x=x.cuda()
			y=y.cuda()
			x+y

____________
##  Autograd: 自动微分

> * pytorch中神经网络的核心是`autograd`包
> * `autograd`可以为所有Tensor上的操作自动求微分
> * 它是一个define-by-run的框架： which means that your backprop is defined by how your code is run

### 1. 变量： Variable

> * `autograd.Variable` 是`autograd`包的核心
> * 它包装了Tensor, 支持Tensor的所有操作
> * 一旦完成了所有的计算，可以调用`.backward()`来计算所有的梯度。
> >*  如果`Variable`是个scalar, 不需要传入任何参数给`backward()`
> > *  否则，需要传入一个`grad_output` 参数, size与Variable的size一致

`autograd.Variable`的属性：
> * 可以通过`.data`属性来访问原始的tensor
> * 而计算的梯度累加到`.grad`属性中
> * `.grad_fn`: 表示创建变量`Variable`的函数，用户创建的除外（`grad_fn is None`）

* 导入torch和Variable

		import torch
		from torch.autograd import Variable
* 创建变量

		x=Variable(torch.ones(2,2),requires_grad=True)
		print(x)
* 在变量上做操作

		y=x+2
		print(y)
* `y`是通过一个operation创建的，所以它有`grad_fn`

		print(y.grad_fn) 
* 更多操作

		z=y*y*3
		out=z.mean()
		print(z,out)

### 2. 梯度： Gradients

*  `out.backward()` 与`out.backward(torch.Tensor([1.0]))` 等价

		out.backward()
		#输出梯度 d(out)/dx
		print(x.grad) 

* 可以用autograd做更疯狂的事情

		x=torch.randn(3)
		x=Variable(x,requires_grad=True)

		y=x*2
		while y.data.norm() < 1000:
			y=y*2
		print(y)
		#计算梯度
		gradients=torch.FloatTensor([0.1,1.0,0.0001])
		y.backward(gradients)
		
		print(x.grad)

> `Variable`和`Function`的文档：[http://pytorch.org/docs/autograd](http://pytorch.org/docs/autograd)

## 神经网络： Neural Networks

> * 神经网络可以使用`torch.nn`包来实现
> * `nn`依赖`autograd`来定义模型和微分
> * `nn.Module` 包含layers和方法`forward(input)` 可以返回`output`
> * 需要定义`forward`函数和`backward`函数
> * `torch.nn`只支持mini-batch的输入，不支持单个样本
> > * 比如，`nn.Conv2d`的输入为4D的tensor: `nSamples * nChannels * Height * Width`
> > * 如果要用单个样本，使用`input.unsqueeze(0)` 添加一个batch 维

### 1. 训练神经网络的典型的过程为：

> * 定义包含可学习参数(weights)的神经网络
> * 在输入数据上迭代
> * 通过网络处理输入
> * 计算loss
> * 梯度反向传播给网络中的参数
> * 更新网络的参数`weights`, 一般使用一个简单的规则：`weight=weight-learning_rate * gradient`

		import torch
		from torch.autograd import Variable
		import torch.nn as nn
		import torch.nn.functional as F
		
		class Net(nn.Moduel):
			def __init__(self):
				super(Net,self).__init__()
				# 输入图像的channel, 6 output channel, 5*5 卷积核
				self.conv1=nn.Conv2d(1,6,5)
				self.conv2=nn.Conv2d(6,16,5)
				#an affine operation: y=Wx+b
				self.fc1=nn.Linear(16*5*5,120)
				self.fc2=nn.Linear(120,84)
				self.fc3=nn.Linear(84,10)
			def forward(self,x):
				#Max pooling over a (2,2) window
				x=F.max_pool2d(F.relu(self.conv1(x)),(2,2))
				#如果size是方阵，可以只使用一个数字
				x=F.max_pool2d(F.relu(self.conv2d(x)),2)
				x=x.view(-1,self.num_flat_features(x))
				x=F.relu(self.fc1(x))
				x=F.relu(self.fc2(x))
				x=self.fc3(x)
				return x
			def num_flat_features(self,x):
				size=x.size()[1:] #除了batch维的所有维
				num_features=1
				for s in size:
					num_features*=s
				return num_features
		
		net=Net()
		print(net)

* `net.parameters()`可以返回模型中的可学习的参数

		params=list(net.parameters())
		print(len(params))
		print(params[0].size()) #conv1的权重参数

* forward的输入是一个变量`autograd.Variable`, 输出也是

	input=Variable(torch.randn(1,1,32,32))
	out=net(input)
	print(out)

### 2. Loss Function

> * `nn`包里有一些不同的损失函数
> * 最简单的一个是`nn.MSELoss`
> * 输入为:(output,target)

		output=net(input)
		target=Vaiable(torch.arange(1,11)) # a dummy target
		criterion=nn.MSELoss()
		loss=criterion(output,target)
		print(loss)

* 使用`.grad_fn`可以得到`loss`的计算图

		input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d
		      -> view -> linear -> relu -> linear -> relu -> linear
		      -> MSELoss
		      -> loss

* 使用`loss.backward()` 计算整个图中关于loss的微分，图中所有的变量都会有`.grad`变量保存累加的梯度

* 为了说明，可以反向跟踪一些步骤：

		print(loss.grad_fn)  # MSELoss
		print(loss.grad_fn.next_functions[0][0])  # Linear
		print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU

输出为：

		<MseLossBackward object at 0x7ff91efb6ba8>
		<AddmmBackward object at 0x7ff91efb6860>
		<ExpandBackward object at 0x7ff91efb6860>

### 3. 反向传播：Backprop

> * 调用`loss.backward()` 反向传播误差
> * 需要先使用`net.zero_grad()`清空梯度，否则梯度会累加到已有的梯度上

* 调用`loss.backward()` 并输出conv1的bias的梯度在backward前后的值：

		net.zero_grad()  # zeroses the gradient buffers of all parameters
		
		print('conv1.bias.grad before backward')
		print(net.conv1.bias.grad)
		
		loss.backward()
		
		print('conv1.bias.grad after backward')
		print(net.conv1.bias.grad)


### 4. 更新参数

 * 最简单的更新梯度的方式是使用随机梯度下降(SGD):
		
		weight=weight-learning_rate*gradient

* 可以使用简单的python代码实现

		learning_rate=0.01
		for f in net.parameters():
			f.data.sub_(f.grad.data * learning_rate)

* 然而，你可能会用到不同的更新规则，比如SGD, Nesterov-SGD, Adam, RMSProp, RMSProp 等
* `torch.optim`中实现了这些方法，使用起来很简单:

		import torch.optim as optim
		
		#创建optimizer
		optimizer=optim.SGD(net.parameters(),lr=0.01)
		
		#in your training loop
		optimizer.zero_grad() #zero the gradient buffers
		output=net(input)
		loss=criterion(output,target)
		loss.backward()
		optimizer.step()

### 5. 在GPU上训练

> * 可以使用`net.cuda()`把神经网络转移到GPU上
> * 注意：必须把输入和输出也送到GPU上

	inputs=Variable(inputs.cuda())
	labels=Vairable(labels.cuda())